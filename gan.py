# -*- coding: utf-8 -*-
"""GAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LRI5gUtBvkxcuh6Enq3V6wJigRjgLjZk
"""

!pip install pytorch-lightning

# ‚öôÔ∏è Install dependencies
!pip install pytorch-lightning --quiet

# üì¶ Imports
import os
import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, random_split
from torchvision.datasets import MNIST
import matplotlib.pyplot as plt
import pytorch_lightning as pl

random_seed = 42
torch.manual_seed(random_seed)
BATCH_SIZE = 128
NUM_WORKERS = os.cpu_count() // 2

class MNISTDataModule(pl.LightningDataModule):
    def __init__(self, data_dir="./data", batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):
        super().__init__()
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])

    def prepare_data(self):
        MNIST(self.data_dir, train=True, download=True)
        MNIST(self.data_dir, train=False, download=True)

    def setup(self, stage=None):
        if stage in ("fit", None):
            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)
            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])
        if stage in ("test", None):
            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)

    def train_dataloader(self):
        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=self.num_workers)

class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Conv2d(1, 10, 5),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(10, 20, 5),
            nn.Dropout2d(),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Flatten(),
            nn.Linear(320, 50),
            nn.ReLU(),
            nn.Dropout(),
            nn.Linear(50, 1)
        )

    def forward(self, x):
        return self.model(x)

class Generator(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(latent_dim, 7 * 7 * 64),
            nn.ReLU(),
            nn.Unflatten(1, (64, 7, 7)),
            nn.ConvTranspose2d(64, 32, 4, stride=2),
            nn.ReLU(),
            nn.ConvTranspose2d(32, 16, 4, stride=2),
            nn.ReLU(),
            nn.Conv2d(16, 1, 7)
        )

    def forward(self, x):
        return self.model(x)

class GAN(pl.LightningModule):
    def __init__(self, latent_dim=100, lr=0.0002):
        super().__init__()
        self.save_hyperparameters()
        self.generator = Generator(latent_dim)
        self.discriminator = Discriminator()
        self.validation_z = torch.randn(25, latent_dim)
        self.automatic_optimization = False

    def forward(self, z):
        return self.generator(z)

    def adversarial_loss(self, y_hat, y):
        return F.binary_cross_entropy_with_logits(y_hat, y)

    def training_step(self, batch, batch_idx):
        opt_g, opt_d = self.optimizers()
        real_imgs, _ = batch
        batch_size = real_imgs.size(0)
        z = torch.randn(batch_size, self.hparams.latent_dim).type_as(real_imgs)

        fake_imgs = self(z)
        y_hat_fake = self.discriminator(fake_imgs)
        g_loss = self.adversarial_loss(y_hat_fake, torch.ones_like(y_hat_fake))
        opt_g.zero_grad()
        self.manual_backward(g_loss)
        opt_g.step()

        y_hat_real = self.discriminator(real_imgs)
        real_loss = self.adversarial_loss(y_hat_real, torch.ones_like(y_hat_real))

        y_hat_fake = self.discriminator(fake_imgs.detach())
        fake_loss = self.adversarial_loss(y_hat_fake, torch.zeros_like(y_hat_fake))

        d_loss = (real_loss + fake_loss) / 2
        opt_d.zero_grad()
        self.manual_backward(d_loss)
        opt_d.step()

        self.log("g_loss", g_loss, prog_bar=True)
        self.log("d_loss", d_loss, prog_bar=True)

    def configure_optimizers(self):
        opt_g = torch.optim.Adam(self.generator.parameters(), lr=self.hparams.lr)
        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=self.hparams.lr)
        return [opt_g, opt_d]

    def on_train_end(self):
        self.plot_generated_images()

    def plot_generated_images(self):
        z = self.validation_z.type_as(self.generator.model[0].weight)
        with torch.no_grad():
            samples = self(z).cpu()

        fig = plt.figure(figsize=(5, 5))
        for i in range(25):
            plt.subplot(5, 5, i + 1)
            plt.imshow(samples[i, 0], cmap="gray_r")
            plt.axis("off")
        plt.suptitle("Generated Digits")
        plt.tight_layout()
        plt.show()

dm = MNISTDataModule()
model = GAN()
trainer = pl.Trainer(
    max_epochs=50,
    accelerator="gpu",
    devices=1,
    precision="16-mixed",
    log_every_n_steps=10
)
trainer.fit(model, dm)

